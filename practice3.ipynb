{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 물체 검출 모델(SSD) 학습하기\n",
    "좋은 성능을 달성하기 위해서 매우 큰 데이터셋(ImageNet 등)에서 영상 분류를 위해서 학습된 모델의 파라미터를 가져와서\n",
    "풀고자 하는 문제에 맞게 학습하는 전이학습(Transfer learning)이 많이 사용됩니다.\n",
    "\n",
    "다음 코드를 활용해서 적당히 학습 된 모델 파라미터(weights/SSD300_early_stop.pth)를 로드하여 \n",
    "PASCAL VOC2007 에서 학습하여 성능을 평가해보도록 합니다.\n",
    "\n",
    "학습이 끝나면, practice2.ipynb를 활용하여 성능을 평가해보고 practice1.ipynb를 활용하여 검출 결과를 그려봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from utils import Timer\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')    \n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD 모델 및 PASCAL VOC2007 데이터 셋 로드\n",
    "학습에 필요한 hyper-parameter 값들을 정의하고, 모델과 데이터셋을 로드합니다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Model: SSD\n",
      "\n",
      "Optimizer: SGD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading the dataset...')\n",
    "\n",
    "resume           = None     # specify filename\n",
    "# resume           = 'weights/ssd300_mAP_77.43_v2.pth'\n",
    "\n",
    "batch_size       = 4\n",
    "max_epochs       = 2\n",
    "lr               = 1e-4\n",
    "momentum         = 0.9\n",
    "weight_decay     = 5e-4\n",
    "lr_schedule      = [int(max_epochs*0.5)]\n",
    "\n",
    "num_classes      = 21\n",
    "\n",
    "overlap_thresh   = 0.5    # overlap threshold to determine positive training samples (default boxes)\n",
    "pos_neg_ratio    = 3      # to alleviate class-imbalance problem (too many negatives), \n",
    "                          # limit the negative samples to 3x of positive samples    \n",
    "    \n",
    "logging_interval = 100\n",
    "\n",
    "dataset = VOCDetection(root=VOC_ROOT, transform=SSDAugmentation(300,MEANS))\n",
    "dataloader = data.DataLoader(dataset, batch_size, num_workers=2, \\\n",
    "                              shuffle=True, collate_fn=detection_collate)\n",
    "    \n",
    "net = build_ssd('vgg16', 300, num_classes)    # initialize SSD\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if resume:\n",
    "    print('Resuming training, loading {}...'.format(resume))\n",
    "    net.load_weights(resume)\n",
    "else:\n",
    "    net.apply(init_weights)\n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "\n",
    "criterion = MultiBoxLoss(num_classes, overlap_thresh, \\\n",
    "                         True, 0, True, pos_neg_ratio, 0.5, False, torch.cuda.is_available())\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_schedule, gamma=0.1)\n",
    "\n",
    "print( 'Model: {}\\n'.format( net.__class__.__name__ ) )\n",
    "print( 'Optimizer: {}\\n'.format( optimizer.__class__.__name__ ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "아래의 코드는 target과 prediction(network output)으로부터 계산된 loss를 이용하여, <br/>\n",
    "loss를 줄여가는 방향의 gradient를 구하고 모델을 업데이트 하는 방식으로 모델을 학습하는 코드입니다. <br/>\n",
    "\n",
    "    1. 데이터 로드 \n",
    "    2. Forward propagation \n",
    "    3. Backward propagation (Gradients 계산)\n",
    "    4. Loss 계산\n",
    "    5. Model update\n",
    "    \n",
    "    \n",
    "[TODO] 아래의 각 과정이 코드에서 어느 부분에 대응되는지 생각해보세요.<br/>\n",
    "[TODO] 아래의 코드에는 심각한 문제가 있습니다. 실행하기 전에 어느 부분이 문제인지 생각해 보세요.<br/>\n",
    "hint: backward 함수가 실행되면서 network에 있는 각각의 weight가 update되어야하는 gradient가 별도의 공간에 저장이 됩니다.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, net, dataloader, cur_lr):\n",
    "    \n",
    "    print('Training SSD')\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # loss counters\n",
    "    total_loss = 0\n",
    "    \n",
    "    sum_loss = 0\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    \n",
    "    # timers\n",
    "    _t = {'forward': Timer(), 'backward': Timer()}    \n",
    "    \n",
    "    # load train data\n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            with torch.no_grad():\n",
    "                targets = [ann.cuda() for ann in targets]\n",
    "                                \n",
    "        _t['forward'].tic()        \n",
    "        out = net(images)\n",
    "        forward_time = _t['forward'].toc(average=True)\n",
    "                \n",
    "        _t['backward'].tic()\n",
    "        \n",
    "        ### [TODO] 뭔가 중요한 스텝이 생략되었네요!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "#         loss *= -0.01        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        backward_time = _t['backward'].toc(average=True)\n",
    "                \n",
    "#         loss *= -100\n",
    "        sum_loss += loss.item()\n",
    "        loc_loss += loss_l.item()\n",
    "        conf_loss += loss_c.item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % logging_interval == 0:            \n",
    "            print('[Epoch {:3d}][iter {:5d}/{:5d}] Loss: {:7.4f} = {:>7.4f}(loc) + {:>7.4f}(cls) \\\n",
    "                  || forward {:4.2f}s, backward {:4.2f}s || lr: {:.2f}'.format(\n",
    "                epoch, batch_idx, len(dataloader), \\\n",
    "                sum_loss/logging_interval, loc_loss/logging_interval, conf_loss/logging_interval, \\\n",
    "                forward_time, backward_time, \\\n",
    "                cur_lr\n",
    "                )\n",
    "            )   \n",
    "            \n",
    "            sum_loss = 0\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 시작\n",
    "max_epochs 만큼 loop을 돌면서 모델을 학습합니다.\n",
    "특정 iteration (in train func.) 또는 epoch 마다 learning rate을 조절하는 learning rate scheduling 도 일반적으로 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SSD\n",
      "[Epoch   0][iter    99/ 4138] Loss:     nan =     nan(loc) +     nan(cls)                   || forward 0.01s, backward 0.04s || lr: 0.00\n",
      "[Epoch   0][iter   199/ 4138] Loss:     nan =     nan(loc) +     nan(cls)                   || forward 0.01s, backward 0.04s || lr: 0.00\n",
      "[Epoch   0][iter   299/ 4138] Loss:     nan =     nan(loc) +     nan(cls)                   || forward 0.01s, backward 0.04s || lr: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-604749302c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moptim_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# torch.save(net.state_dict(), 'weights/ssd300_mAP_73.22.pth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weights/ssd300_epoch_{:03d}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-3b34d2299408>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, net, dataloader, cur_lr)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mloc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mconf_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    train(epoch, net, dataloader, optim_scheduler.get_lr()[0])\n",
    "    optim_scheduler.step()\n",
    "    # torch.save(net.state_dict(), 'weights/ssd300_mAP_73.22.pth')\n",
    "    torch.save(net.state_dict(), 'weights/ssd300_epoch_{:03d}.pth'.format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
