{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 물체 검출 모델(SSD) 학습하기\n",
    "좋은 성능을 달성하기 위해서<br/>\n",
    "매우 큰 데이터셋(ImageNet 등)에서 영상 분류를 위한 모델을 학습하고,<br/>\n",
    "학습된 모델의 weights를 가져와서 풀고자 하는 문제에 맞게 학습하는 전이학습(Transfer learning)이 많이 사용됩니다.\n",
    "\n",
    "다음 코드에서는 이와 비슷하게, 네트워크 대부분은 잘 학습되어 있고 일부만 학습되어있지 않은 경우를 다루게 됩니다.<br/>\n",
    "즉, Feature extractor의 역할을 하는 VGG16 부분은 제외하고, Classifier의 역할을 하는 layer들만 학습을 진행함으로써,<br/>\n",
    "짧은 시간동안의 학습으로 성능을 개선하는 실험입니다.\n",
    "\n",
    "먼저 학습 된 모델 파라미터(weights/SSD300_early_stop.pth)를 VGG16에 해당하는 부분만 로드하고,<br/>\n",
    "나머지 layer들은 PASCAL VOC2007 에서 학습하여 성능을 평가해보도록 합니다.\n",
    "\n",
    "학습이 끝나면, practice2.ipynb를 활용하여 성능을 평가해보고 practice1.ipynb를 활용하여 검출 결과를 그려봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from utils import Timer\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')    \n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD 모델 및 PASCAL VOC2007 데이터 셋 로드\n",
    "학습에 필요한 hyper-parameter 값들을 정의하고, 모델과 데이터셋을 로드합니다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Resuming training, loading weights/ssd300_before_optimize.pth...\n",
      "Loading weights into state dict...\n",
      "Finished!\n",
      "Model: SSD\n",
      "\n",
      "SSD(\n",
      "  (vgg): ModuleList(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "    (32): ReLU(inplace)\n",
      "    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (34): ReLU(inplace)\n",
      "  )\n",
      "  (L2Norm): L2Norm()\n",
      "  (extras): ModuleList(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Optimizer: SGD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading the dataset...')\n",
    "\n",
    "# resume           = None     # specify filename\n",
    "resume           = 'weights/ssd300_before_optimize.pth'\n",
    "# resume           = 'weights/ssd300_before_optimize.v2.pth'\n",
    "# resume           = 'weights/ssd300_mAP_77.43_v2.pth'\n",
    "\n",
    "batch_size       = 4\n",
    "max_epochs       = 4\n",
    "lr               = 1e-3\n",
    "momentum         = 0.9\n",
    "weight_decay     = 5e-4\n",
    "lr_schedule      = [int(max_epochs*0.5)]\n",
    "\n",
    "num_classes      = 21\n",
    "\n",
    "overlap_thresh   = 0.5    # overlap threshold to determine positive training samples (default boxes)\n",
    "pos_neg_ratio    = 3      # to alleviate class-imbalance problem (too many negatives), \n",
    "                          # limit the negative samples to 3x of positive samples    \n",
    "    \n",
    "logging_interval = 100\n",
    "\n",
    "dataset = VOCDetection(root=VOC_ROOT, transform=SSDAugmentation(300,MEANS))\n",
    "dataloader = data.DataLoader(dataset, batch_size, num_workers=2, \\\n",
    "                              shuffle=True, collate_fn=detection_collate)\n",
    "    \n",
    "net = build_ssd('vgg16', 300, num_classes)    # initialize SSD\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if resume:\n",
    "    print('Resuming training, loading {}...'.format(resume))\n",
    "\n",
    "    net.load_weights(resume)    # load all weights    \n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for param in net.parameters():\n",
    "#             param.add_(torch.randn(param.size()) * 0.001)\n",
    "        \n",
    "#     torch.save(net.state_dict(), 'weights/ssd300_before_optimize.v2.pth'.format(epoch))\n",
    "\n",
    "else:\n",
    "    net.apply(init_weights)\n",
    "        \n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "\n",
    "criterion = MultiBoxLoss(num_classes, overlap_thresh, \\\n",
    "                         True, 0, True, pos_neg_ratio, 0.5, False, torch.cuda.is_available())\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_schedule, gamma=0.1)\n",
    "\n",
    "print( 'Model: {}\\n'.format( net.__class__.__name__ ) )\n",
    "print( net )\n",
    "print( 'Optimizer: {}\\n'.format( optimizer.__class__.__name__ ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "아래의 코드는 target과 prediction(network output)으로부터 계산된 loss를 이용하여, <br/>\n",
    "loss를 줄여가는 방향의 gradient를 구하고 모델을 업데이트 하는 방식으로 모델을 학습하는 코드입니다. <br/>\n",
    "\n",
    "    1. 데이터 로드 \n",
    "    2. Forward propagation \n",
    "    3. Backward propagation (Gradients 계산)\n",
    "    4. Loss 계산\n",
    "    5. Model update\n",
    "    \n",
    "    \n",
    "[TODO] 아래의 각 과정이 코드에서 어느 부분에 대응되는지 생각해보세요.<br/>\n",
    "[TODO] 아래의 코드에는 심각한 문제가 있습니다. 실행하기 전에 어느 부분이 문제인지 생각해 보세요.<br/>\n",
    "hint: backward 함수가 실행되면서 network에 있는 각각의 weight가 update되어야하는 gradient가 별도의 공간에 저장이 됩니다.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, net, dataloader, cur_lr):\n",
    "    \n",
    "    print('Training SSD')\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # loss counters\n",
    "    total_loss = 0\n",
    "    \n",
    "    sum_loss = 0\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    \n",
    "    # timers\n",
    "    _t = {'forward': Timer(), 'backward': Timer()}    \n",
    "    \n",
    "    # load train data\n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            with torch.no_grad():\n",
    "                targets = [ann.cuda() for ann in targets]\n",
    "                                \n",
    "        _t['forward'].tic()        \n",
    "        out = net(images)\n",
    "        forward_time = _t['forward'].toc(average=True)\n",
    "                \n",
    "        _t['backward'].tic()\n",
    "        \n",
    "        ### [TODO] 뭔가 중요한 스텝이 생략되었네요!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        backward_time = _t['backward'].toc(average=True)\n",
    "                \n",
    "        sum_loss += loss.item()\n",
    "        loc_loss += loss_l.item()\n",
    "        conf_loss += loss_c.item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % logging_interval == 0:            \n",
    "            print('[Epoch {:3d}][iter {:5d}/{:5d}] Loss: {:7.4f} = {:>7.4f}(loc) + {:>7.4f}(cls) \\\n",
    "                  || forward {:4.2f}s, backward {:4.2f}s || lr: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(dataloader), \\\n",
    "                sum_loss/logging_interval, loc_loss/logging_interval, conf_loss/logging_interval, \\\n",
    "                forward_time, backward_time, \\\n",
    "                cur_lr\n",
    "                )\n",
    "            )   \n",
    "            \n",
    "            sum_loss = 0\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 시작\n",
    "max_epochs 만큼 loop을 돌면서 모델을 학습합니다.\n",
    "특정 iteration (in train func.) 또는 epoch 마다 learning rate을 조절하는 learning rate scheduling 도 일반적으로 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SSD\n",
      "[Epoch   0][iter    99/ 4138] Loss:  6.7988 =  1.7929(loc) +  5.0059(cls)                   || forward 0.03s, backward 0.09s || lr: 0.001000\n",
      "[Epoch   0][iter   199/ 4138] Loss:  6.3203 =  1.7186(loc) +  4.6017(cls)                   || forward 0.02s, backward 0.07s || lr: 0.001000\n",
      "[Epoch   0][iter   299/ 4138] Loss:  5.7135 =  1.5399(loc) +  4.1737(cls)                   || forward 0.02s, backward 0.07s || lr: 0.001000\n",
      "[Epoch   0][iter   399/ 4138] Loss:  5.4105 =  1.5274(loc) +  3.8831(cls)                   || forward 0.01s, backward 0.07s || lr: 0.001000\n",
      "[Epoch   0][iter   499/ 4138] Loss:  5.1383 =  1.3485(loc) +  3.7898(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter   599/ 4138] Loss:  5.3023 =  1.4729(loc) +  3.8294(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter   699/ 4138] Loss:  5.1784 =  1.4336(loc) +  3.7448(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter   799/ 4138] Loss:  5.0044 =  1.3840(loc) +  3.6205(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter   899/ 4138] Loss:  4.8269 =  1.3184(loc) +  3.5085(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter   999/ 4138] Loss:  5.0760 =  1.3942(loc) +  3.6818(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1099/ 4138] Loss:  5.0371 =  1.4214(loc) +  3.6157(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1199/ 4138] Loss:  4.8896 =  1.3966(loc) +  3.4930(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1299/ 4138] Loss:  5.0777 =  1.4074(loc) +  3.6702(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1399/ 4138] Loss:  5.0574 =  1.4218(loc) +  3.6357(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1499/ 4138] Loss:  4.8557 =  1.3145(loc) +  3.5411(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1599/ 4138] Loss:  4.7152 =  1.3048(loc) +  3.4104(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1699/ 4138] Loss:  4.8159 =  1.3442(loc) +  3.4717(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1799/ 4138] Loss:  4.7505 =  1.3230(loc) +  3.4275(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1899/ 4138] Loss:  4.5396 =  1.2463(loc) +  3.2934(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  1999/ 4138] Loss:  4.7537 =  1.3766(loc) +  3.3771(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2099/ 4138] Loss:  4.5457 =  1.2871(loc) +  3.2585(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2199/ 4138] Loss:  4.8079 =  1.3879(loc) +  3.4200(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2299/ 4138] Loss:  4.5431 =  1.2580(loc) +  3.2851(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2399/ 4138] Loss:  4.6533 =  1.2749(loc) +  3.3784(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2499/ 4138] Loss:  4.4747 =  1.2108(loc) +  3.2639(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2599/ 4138] Loss:  4.7776 =  1.3614(loc) +  3.4162(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2699/ 4138] Loss:  4.4387 =  1.2410(loc) +  3.1978(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2799/ 4138] Loss:  4.4276 =  1.2357(loc) +  3.1919(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2899/ 4138] Loss:  4.3956 =  1.2375(loc) +  3.1581(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  2999/ 4138] Loss:  4.4291 =  1.2476(loc) +  3.1815(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3099/ 4138] Loss:  4.4149 =  1.2260(loc) +  3.1889(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3199/ 4138] Loss:  4.4198 =  1.2703(loc) +  3.1496(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3299/ 4138] Loss:  4.3617 =  1.2469(loc) +  3.1148(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3399/ 4138] Loss:  4.3100 =  1.2153(loc) +  3.0946(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3499/ 4138] Loss:  4.5483 =  1.3095(loc) +  3.2388(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3599/ 4138] Loss:  4.5580 =  1.2429(loc) +  3.3151(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3699/ 4138] Loss:  4.3885 =  1.2644(loc) +  3.1241(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3799/ 4138] Loss:  4.4547 =  1.2137(loc) +  3.2411(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3899/ 4138] Loss:  4.3244 =  1.2247(loc) +  3.0997(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  3999/ 4138] Loss:  4.4763 =  1.2990(loc) +  3.1773(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   0][iter  4099/ 4138] Loss:  4.5867 =  1.2700(loc) +  3.3167(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "Training SSD\n",
      "[Epoch   1][iter    99/ 4138] Loss:  4.1691 =  1.1930(loc) +  2.9761(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter   199/ 4138] Loss:  4.2118 =  1.1934(loc) +  3.0184(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter   299/ 4138] Loss:  4.4234 =  1.2409(loc) +  3.1825(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter   399/ 4138] Loss:  4.3236 =  1.2053(loc) +  3.1183(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter   499/ 4138] Loss:  4.6101 =  1.2935(loc) +  3.3166(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter   599/ 4138] Loss:  4.2766 =  1.1963(loc) +  3.0804(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter   699/ 4138] Loss:  4.1089 =  1.1577(loc) +  2.9512(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter   799/ 4138] Loss:  4.3060 =  1.2167(loc) +  3.0893(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter   899/ 4138] Loss:  4.1774 =  1.1529(loc) +  3.0245(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter   999/ 4138] Loss:  4.3376 =  1.1921(loc) +  3.1455(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1099/ 4138] Loss:  4.4634 =  1.2249(loc) +  3.2384(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1199/ 4138] Loss:  4.3879 =  1.2661(loc) +  3.1219(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1299/ 4138] Loss:  4.3230 =  1.1749(loc) +  3.1481(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1399/ 4138] Loss:  4.2804 =  1.2368(loc) +  3.0436(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1499/ 4138] Loss:  4.1986 =  1.1754(loc) +  3.0232(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1599/ 4138] Loss:  4.3403 =  1.2373(loc) +  3.1030(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1699/ 4138] Loss:  4.5551 =  1.3406(loc) +  3.2146(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1799/ 4138] Loss:  4.2550 =  1.2519(loc) +  3.0031(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1899/ 4138] Loss:  4.0925 =  1.1688(loc) +  2.9237(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  1999/ 4138] Loss:  4.2061 =  1.1846(loc) +  3.0215(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2099/ 4138] Loss:  3.9062 =  1.0984(loc) +  2.8078(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2199/ 4138] Loss:  4.2409 =  1.2045(loc) +  3.0364(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2299/ 4138] Loss:  4.2353 =  1.1827(loc) +  3.0526(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2399/ 4138] Loss:  4.2562 =  1.2063(loc) +  3.0499(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2499/ 4138] Loss:  4.3976 =  1.2147(loc) +  3.1830(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2599/ 4138] Loss:  4.2688 =  1.2030(loc) +  3.0659(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2699/ 4138] Loss:  4.4209 =  1.2236(loc) +  3.1973(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2799/ 4138] Loss:  4.2799 =  1.2671(loc) +  3.0128(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2899/ 4138] Loss:  4.1765 =  1.1713(loc) +  3.0052(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  2999/ 4138] Loss:  4.0528 =  1.1478(loc) +  2.9050(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3099/ 4138] Loss:  4.2177 =  1.1615(loc) +  3.0562(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3199/ 4138] Loss:  4.0568 =  1.1805(loc) +  2.8763(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3299/ 4138] Loss:  4.2761 =  1.2256(loc) +  3.0505(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3399/ 4138] Loss:  4.3362 =  1.2101(loc) +  3.1261(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3499/ 4138] Loss:  4.0942 =  1.1373(loc) +  2.9569(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3599/ 4138] Loss:  4.0867 =  1.1243(loc) +  2.9625(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3699/ 4138] Loss:  4.2845 =  1.2323(loc) +  3.0522(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3799/ 4138] Loss:  4.2418 =  1.2100(loc) +  3.0318(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3899/ 4138] Loss:  4.2276 =  1.1960(loc) +  3.0316(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  3999/ 4138] Loss:  4.2287 =  1.1979(loc) +  3.0308(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   1][iter  4099/ 4138] Loss:  4.1823 =  1.1359(loc) +  3.0464(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "Training SSD\n",
      "[Epoch   2][iter    99/ 4138] Loss:  4.1927 =  1.1686(loc) +  3.0241(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter   199/ 4138] Loss:  3.9761 =  1.1013(loc) +  2.8747(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter   299/ 4138] Loss:  4.1456 =  1.1865(loc) +  2.9591(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter   399/ 4138] Loss:  3.9807 =  1.1371(loc) +  2.8436(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter   499/ 4138] Loss:  3.9522 =  1.1497(loc) +  2.8025(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter   599/ 4138] Loss:  4.2216 =  1.2250(loc) +  2.9966(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter   699/ 4138] Loss:  4.2114 =  1.2363(loc) +  2.9751(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter   799/ 4138] Loss:  4.0902 =  1.1856(loc) +  2.9046(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter   899/ 4138] Loss:  3.9667 =  1.0880(loc) +  2.8787(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter   999/ 4138] Loss:  4.1196 =  1.1918(loc) +  2.9278(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  1099/ 4138] Loss:  3.9037 =  1.1138(loc) +  2.7900(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  1199/ 4138] Loss:  4.0363 =  1.1688(loc) +  2.8675(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  1299/ 4138] Loss:  3.9452 =  1.1375(loc) +  2.8077(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  1399/ 4138] Loss:  4.0843 =  1.1652(loc) +  2.9191(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  1499/ 4138] Loss:  4.2389 =  1.2128(loc) +  3.0261(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  1599/ 4138] Loss:  4.1163 =  1.1485(loc) +  2.9678(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  1699/ 4138] Loss:  4.2098 =  1.1885(loc) +  3.0212(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  1799/ 4138] Loss:  4.2809 =  1.2109(loc) +  3.0700(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  1899/ 4138] Loss:  4.0674 =  1.1316(loc) +  2.9358(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  1999/ 4138] Loss:  3.9918 =  1.1463(loc) +  2.8455(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  2099/ 4138] Loss:  4.2501 =  1.1793(loc) +  3.0708(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  2199/ 4138] Loss:  4.0586 =  1.1708(loc) +  2.8877(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  2299/ 4138] Loss:  4.0052 =  1.1187(loc) +  2.8865(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  2399/ 4138] Loss:  4.0933 =  1.1401(loc) +  2.9532(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  2499/ 4138] Loss:  3.9292 =  1.0960(loc) +  2.8332(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  2599/ 4138] Loss:  3.9159 =  1.1013(loc) +  2.8146(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  2699/ 4138] Loss:  3.9862 =  1.1686(loc) +  2.8176(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  2799/ 4138] Loss:  4.0110 =  1.1240(loc) +  2.8870(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  2899/ 4138] Loss:  4.0957 =  1.1999(loc) +  2.8958(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  2999/ 4138] Loss:  3.9381 =  1.1656(loc) +  2.7725(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  3099/ 4138] Loss:  4.1959 =  1.1439(loc) +  3.0520(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  3199/ 4138] Loss:  4.0383 =  1.1440(loc) +  2.8943(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   2][iter  3299/ 4138] Loss:  4.2231 =  1.1463(loc) +  3.0768(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  3399/ 4138] Loss:  4.0438 =  1.1550(loc) +  2.8887(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  3499/ 4138] Loss:  3.9747 =  1.1175(loc) +  2.8572(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  3599/ 4138] Loss:  4.0314 =  1.1401(loc) +  2.8913(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  3699/ 4138] Loss:  4.1202 =  1.1521(loc) +  2.9680(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  3799/ 4138] Loss:  3.8665 =  1.1480(loc) +  2.7185(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  3899/ 4138] Loss:  4.1538 =  1.1221(loc) +  3.0317(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  3999/ 4138] Loss:  4.0917 =  1.1060(loc) +  2.9857(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "[Epoch   2][iter  4099/ 4138] Loss:  4.1234 =  1.1349(loc) +  2.9884(cls)                   || forward 0.01s, backward 0.06s || lr: 0.001000\n",
      "Training SSD\n",
      "[Epoch   3][iter    99/ 4138] Loss:  3.8894 =  1.1118(loc) +  2.7776(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter   199/ 4138] Loss:  3.6731 =  1.0648(loc) +  2.6083(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter   299/ 4138] Loss:  3.3800 =  0.9521(loc) +  2.4278(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter   399/ 4138] Loss:  3.4928 =  0.9691(loc) +  2.5237(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter   499/ 4138] Loss:  3.3603 =  0.9444(loc) +  2.4159(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter   599/ 4138] Loss:  3.2274 =  0.9090(loc) +  2.3183(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter   699/ 4138] Loss:  3.4018 =  0.9805(loc) +  2.4213(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter   799/ 4138] Loss:  3.3310 =  0.9199(loc) +  2.4111(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter   899/ 4138] Loss:  3.2654 =  0.8980(loc) +  2.3674(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter   999/ 4138] Loss:  3.2066 =  0.9014(loc) +  2.3052(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1099/ 4138] Loss:  3.3559 =  0.9511(loc) +  2.4049(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1199/ 4138] Loss:  3.3609 =  0.9288(loc) +  2.4321(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1299/ 4138] Loss:  3.2977 =  0.9467(loc) +  2.3511(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1399/ 4138] Loss:  3.3946 =  0.9796(loc) +  2.4150(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1499/ 4138] Loss:  3.3225 =  0.9675(loc) +  2.3550(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1599/ 4138] Loss:  3.2791 =  0.9161(loc) +  2.3630(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1699/ 4138] Loss:  3.2646 =  0.9578(loc) +  2.3069(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1799/ 4138] Loss:  3.1634 =  0.9377(loc) +  2.2257(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1899/ 4138] Loss:  3.3361 =  0.9382(loc) +  2.3978(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  1999/ 4138] Loss:  3.2156 =  0.9373(loc) +  2.2784(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2099/ 4138] Loss:  3.3738 =  0.9845(loc) +  2.3893(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2199/ 4138] Loss:  3.1704 =  0.9260(loc) +  2.2444(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2299/ 4138] Loss:  3.0628 =  0.8550(loc) +  2.2078(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2399/ 4138] Loss:  3.0684 =  0.8431(loc) +  2.2254(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2499/ 4138] Loss:  3.0933 =  0.8913(loc) +  2.2020(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2599/ 4138] Loss:  3.1808 =  0.9535(loc) +  2.2272(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2699/ 4138] Loss:  3.1867 =  0.8925(loc) +  2.2942(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2799/ 4138] Loss:  3.0198 =  0.8549(loc) +  2.1650(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2899/ 4138] Loss:  3.1293 =  0.8740(loc) +  2.2553(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  2999/ 4138] Loss:  3.4743 =  1.0089(loc) +  2.4653(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3099/ 4138] Loss:  3.2305 =  0.9401(loc) +  2.2905(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3199/ 4138] Loss:  3.0107 =  0.8564(loc) +  2.1542(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3299/ 4138] Loss:  3.1183 =  0.9005(loc) +  2.2178(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3399/ 4138] Loss:  3.0748 =  0.8762(loc) +  2.1986(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3499/ 4138] Loss:  3.1128 =  0.9358(loc) +  2.1771(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3599/ 4138] Loss:  3.1787 =  0.9266(loc) +  2.2521(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3699/ 4138] Loss:  3.0461 =  0.8679(loc) +  2.1782(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3799/ 4138] Loss:  3.0429 =  0.8838(loc) +  2.1592(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3899/ 4138] Loss:  3.0317 =  0.8767(loc) +  2.1551(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  3999/ 4138] Loss:  3.0719 =  0.8507(loc) +  2.2212(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   3][iter  4099/ 4138] Loss:  3.1188 =  0.8712(loc) +  2.2476(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    optim_scheduler.step()\n",
    "    train(epoch, net, dataloader, optim_scheduler.get_lr()[0])    \n",
    "    torch.save(net.state_dict(), 'weights/ssd300_epoch_{:03d}.v2.pth'.format(epoch))\n",
    "    \n",
    "print('done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
