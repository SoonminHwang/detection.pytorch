{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 물체 검출 모델(SSD) 학습하기\n",
    "좋은 성능을 달성하기 위해서<br/>\n",
    "매우 큰 데이터셋(ImageNet 등)에서 영상 분류를 위한 모델을 학습하고,<br/>\n",
    "학습된 모델의 weights를 가져와서 풀고자 하는 문제에 맞게 학습하는 전이학습(Transfer learning)이 많이 사용됩니다.\n",
    "\n",
    "다음 코드에서는 이와 비슷하게, 네트워크 대부분은 잘 학습되어 있고 일부만 학습되어있지 않은 경우를 다루게 됩니다.<br/>\n",
    "즉, Feature extractor의 역할을 하는 VGG16 부분은 제외하고, Classifier의 역할을 하는 layer들만 학습을 진행함으로써,<br/>\n",
    "짧은 시간동안의 학습으로 성능을 개선하는 실험입니다.\n",
    "\n",
    "먼저 학습 된 모델 파라미터(weights/SSD300_early_stop.pth)를 VGG16에 해당하는 부분만 로드하고,<br/>\n",
    "나머지 layer들은 PASCAL VOC2007 에서 학습하여 성능을 평가해보도록 합니다.\n",
    "\n",
    "학습이 끝나면, practice2.ipynb를 활용하여 성능을 평가해보고 practice1.ipynb를 활용하여 검출 결과를 그려봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from utils import Timer\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')    \n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD 모델 및 PASCAL VOC2007 데이터 셋 로드\n",
    "학습에 필요한 hyper-parameter 값들을 정의하고, 모델과 데이터셋을 로드합니다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Resuming training, loading weights/ssd300_before_optimize.pth...\n",
      "Loading weights into state dict...\n",
      "Finished!\n",
      "Model: SSD\n",
      "\n",
      "SSD(\n",
      "  (vgg): ModuleList(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "    (32): ReLU(inplace)\n",
      "    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (34): ReLU(inplace)\n",
      "  )\n",
      "  (L2Norm): L2Norm()\n",
      "  (extras): ModuleList(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Optimizer: SGD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading the dataset...')\n",
    "\n",
    "# resume           = None     # specify filename\n",
    "resume           = 'weights/ssd300_before_optimize.pth'\n",
    "# resume           = 'weights/ssd300_mAP_77.43_v2.pth'\n",
    "\n",
    "batch_size       = 4\n",
    "max_epochs       = 2\n",
    "lr               = 1e-3\n",
    "momentum         = 0.9\n",
    "weight_decay     = 5e-4\n",
    "lr_schedule      = [int(max_epochs*0.5)]\n",
    "\n",
    "num_classes      = 21\n",
    "\n",
    "overlap_thresh   = 0.5    # overlap threshold to determine positive training samples (default boxes)\n",
    "pos_neg_ratio    = 3      # to alleviate class-imbalance problem (too many negatives), \n",
    "                          # limit the negative samples to 3x of positive samples    \n",
    "    \n",
    "logging_interval = 100\n",
    "\n",
    "dataset = VOCDetection(root=VOC_ROOT, transform=SSDAugmentation(300,MEANS))\n",
    "dataloader = data.DataLoader(dataset, batch_size, num_workers=2, \\\n",
    "                              shuffle=True, collate_fn=detection_collate)\n",
    "    \n",
    "net = build_ssd('vgg16', 300, num_classes)    # initialize SSD\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if resume:\n",
    "    print('Resuming training, loading {}...'.format(resume))\n",
    "    net.load_weights(resume)    # load all weights    \n",
    "    \n",
    "else:\n",
    "    net.apply(init_weights)\n",
    "        \n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "\n",
    "criterion = MultiBoxLoss(num_classes, overlap_thresh, \\\n",
    "                         True, 0, True, pos_neg_ratio, 0.5, False, torch.cuda.is_available())\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_schedule, gamma=0.1)\n",
    "\n",
    "print( 'Model: {}\\n'.format( net.__class__.__name__ ) )\n",
    "print( net )\n",
    "print( 'Optimizer: {}\\n'.format( optimizer.__class__.__name__ ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "아래의 코드는 target과 prediction(network output)으로부터 계산된 loss를 이용하여, <br/>\n",
    "loss를 줄여가는 방향의 gradient를 구하고 모델을 업데이트 하는 방식으로 모델을 학습하는 코드입니다. <br/>\n",
    "\n",
    "    1. 데이터 로드 \n",
    "    2. Forward propagation \n",
    "    3. Backward propagation (Gradients 계산)\n",
    "    4. Loss 계산\n",
    "    5. Model update\n",
    "    \n",
    "    \n",
    "[TODO] 아래의 각 과정이 코드에서 어느 부분에 대응되는지 생각해보세요.<br/>\n",
    "[TODO] 아래의 코드에는 심각한 문제가 있습니다. 실행하기 전에 어느 부분이 문제인지 생각해 보세요.<br/>\n",
    "hint: backward 함수가 실행되면서 network에 있는 각각의 weight가 update되어야하는 gradient가 누.적.해.서. 저장이 됩니다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, net, dataloader, cur_lr):\n",
    "    \n",
    "    print('Training SSD')\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # loss counters\n",
    "    total_loss = 0\n",
    "    \n",
    "    sum_loss = 0\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    \n",
    "    # timers\n",
    "    _t = {'forward': Timer(), 'backward': Timer()}    \n",
    "    \n",
    "    # load train data\n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            with torch.no_grad():\n",
    "                targets = [ann.cuda() for ann in targets]\n",
    "                                \n",
    "        _t['forward'].tic()        \n",
    "        out = net(images)\n",
    "        forward_time = _t['forward'].toc(average=True)\n",
    "                \n",
    "        _t['backward'].tic()\n",
    "        \n",
    "        ### [TODO] 뭔가 중요한 스텝이 생략되었네요!\n",
    "        \n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        backward_time = _t['backward'].toc(average=True)\n",
    "                \n",
    "        sum_loss += loss.item()\n",
    "        loc_loss += loss_l.item()\n",
    "        conf_loss += loss_c.item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % logging_interval == 0:            \n",
    "            print('[Epoch {:3d}][iter {:5d}/{:5d}] Loss: {:7.4f} = {:>7.4f}(loc) + {:>7.4f}(cls) \\\n",
    "                  || forward {:4.2f}s, backward {:4.2f}s || lr: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(dataloader), \\\n",
    "                sum_loss/logging_interval, loc_loss/logging_interval, conf_loss/logging_interval, \\\n",
    "                forward_time, backward_time, \\\n",
    "                cur_lr\n",
    "                )\n",
    "            )   \n",
    "            \n",
    "            sum_loss = 0\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 시작\n",
    "max_epochs 만큼 loop을 돌면서 모델을 학습합니다.\n",
    "특정 iteration (in train func.) 또는 epoch 마다 learning rate을 조절하는 learning rate scheduling 도 일반적으로 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SSD\n",
      "[Epoch   0][iter    99/ 4138] Loss:  6.7632 =  1.7997(loc) +  4.9635(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter   199/ 4138] Loss:  6.1826 =  1.6623(loc) +  4.5203(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter   299/ 4138] Loss:  5.5730 =  1.4713(loc) +  4.1017(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter   399/ 4138] Loss:  5.5508 =  1.4552(loc) +  4.0956(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter   499/ 4138] Loss:  5.5183 =  1.4700(loc) +  4.0483(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter   599/ 4138] Loss:  5.3081 =  1.5337(loc) +  3.7744(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter   699/ 4138] Loss:  5.1419 =  1.3747(loc) +  3.7672(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter   799/ 4138] Loss:  5.2202 =  1.4484(loc) +  3.7718(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter   899/ 4138] Loss:  4.7394 =  1.3126(loc) +  3.4268(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter   999/ 4138] Loss:  5.1316 =  1.4202(loc) +  3.7114(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1099/ 4138] Loss:  5.0295 =  1.3577(loc) +  3.6718(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1199/ 4138] Loss:  4.8274 =  1.3357(loc) +  3.4916(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1299/ 4138] Loss:  5.1523 =  1.4332(loc) +  3.7191(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1399/ 4138] Loss:  4.8495 =  1.3639(loc) +  3.4857(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1499/ 4138] Loss:  4.5508 =  1.2599(loc) +  3.2909(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1599/ 4138] Loss:  4.8814 =  1.3617(loc) +  3.5197(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1699/ 4138] Loss:  4.8490 =  1.3400(loc) +  3.5090(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1799/ 4138] Loss:  4.7869 =  1.3400(loc) +  3.4469(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1899/ 4138] Loss:  4.7962 =  1.3474(loc) +  3.4487(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  1999/ 4138] Loss:  4.5249 =  1.2576(loc) +  3.2673(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2099/ 4138] Loss:  4.5943 =  1.2645(loc) +  3.3298(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2199/ 4138] Loss:  4.4125 =  1.2353(loc) +  3.1772(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2299/ 4138] Loss:  4.8739 =  1.3873(loc) +  3.4866(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2399/ 4138] Loss:  4.7355 =  1.3700(loc) +  3.3655(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2499/ 4138] Loss:  4.5360 =  1.2361(loc) +  3.2999(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2599/ 4138] Loss:  4.5102 =  1.2397(loc) +  3.2705(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2699/ 4138] Loss:  4.7739 =  1.3724(loc) +  3.4014(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2799/ 4138] Loss:  4.4017 =  1.2371(loc) +  3.1646(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2899/ 4138] Loss:  4.5872 =  1.3201(loc) +  3.2672(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  2999/ 4138] Loss:  4.5930 =  1.3123(loc) +  3.2807(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  3099/ 4138] Loss:  4.6574 =  1.3121(loc) +  3.3453(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  3199/ 4138] Loss:  4.4824 =  1.2291(loc) +  3.2533(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  3299/ 4138] Loss:  4.5194 =  1.2242(loc) +  3.2952(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  3399/ 4138] Loss:  4.6113 =  1.3073(loc) +  3.3040(cls)                   || forward 0.01s, backward 0.04s || lr: 0.001000\n",
      "[Epoch   0][iter  3499/ 4138] Loss:  4.4979 =  1.2547(loc) +  3.2432(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   0][iter  3599/ 4138] Loss:  4.3435 =  1.2266(loc) +  3.1169(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   0][iter  3699/ 4138] Loss:  4.4832 =  1.2363(loc) +  3.2469(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   0][iter  3799/ 4138] Loss:  4.4651 =  1.2600(loc) +  3.2051(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   0][iter  3899/ 4138] Loss:  4.5845 =  1.3230(loc) +  3.2615(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   0][iter  3999/ 4138] Loss:  4.6375 =  1.3426(loc) +  3.2948(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "[Epoch   0][iter  4099/ 4138] Loss:  4.3194 =  1.2509(loc) +  3.0685(cls)                   || forward 0.01s, backward 0.05s || lr: 0.001000\n",
      "Training SSD\n",
      "[Epoch   1][iter    99/ 4138] Loss:  4.0062 =  1.1361(loc) +  2.8701(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter   199/ 4138] Loss:  3.8844 =  1.0996(loc) +  2.7849(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   1][iter   299/ 4138] Loss:  3.8124 =  1.1116(loc) +  2.7008(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter   399/ 4138] Loss:  3.6153 =  1.0261(loc) +  2.5891(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter   499/ 4138] Loss:  3.3956 =  0.9755(loc) +  2.4201(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   1][iter   599/ 4138] Loss:  3.4862 =  1.0221(loc) +  2.4641(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   1][iter   699/ 4138] Loss:  3.5134 =  0.9719(loc) +  2.5415(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   1][iter   799/ 4138] Loss:  3.5812 =  1.0186(loc) +  2.5626(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   1][iter   899/ 4138] Loss:  3.4822 =  1.0141(loc) +  2.4680(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   1][iter   999/ 4138] Loss:  3.4610 =  1.0067(loc) +  2.4543(cls)                   || forward 0.01s, backward 0.06s || lr: 0.000100\n",
      "[Epoch   1][iter  1099/ 4138] Loss:  3.3104 =  0.9611(loc) +  2.3493(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  1199/ 4138] Loss:  3.3621 =  0.9575(loc) +  2.4046(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  1299/ 4138] Loss:  3.3379 =  0.9371(loc) +  2.4008(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  1399/ 4138] Loss:  3.4051 =  1.0054(loc) +  2.3997(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  1499/ 4138] Loss:  3.3484 =  0.9270(loc) +  2.4214(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  1599/ 4138] Loss:  3.4791 =  1.0215(loc) +  2.4576(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  1699/ 4138] Loss:  3.2958 =  0.9571(loc) +  2.3386(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  1799/ 4138] Loss:  3.2014 =  0.9539(loc) +  2.2475(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  1899/ 4138] Loss:  3.2741 =  0.9581(loc) +  2.3160(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  1999/ 4138] Loss:  3.3321 =  0.9780(loc) +  2.3541(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2099/ 4138] Loss:  3.3039 =  0.9470(loc) +  2.3569(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2199/ 4138] Loss:  3.1653 =  0.9067(loc) +  2.2586(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2299/ 4138] Loss:  3.3464 =  0.9634(loc) +  2.3830(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2399/ 4138] Loss:  3.2413 =  0.9016(loc) +  2.3396(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2499/ 4138] Loss:  3.3313 =  0.9934(loc) +  2.3379(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2599/ 4138] Loss:  3.2678 =  0.9365(loc) +  2.3312(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2699/ 4138] Loss:  3.3287 =  0.9540(loc) +  2.3747(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2799/ 4138] Loss:  3.1485 =  0.9005(loc) +  2.2481(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2899/ 4138] Loss:  3.2693 =  0.9318(loc) +  2.3375(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  2999/ 4138] Loss:  3.1694 =  0.8963(loc) +  2.2731(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3099/ 4138] Loss:  3.2963 =  0.9376(loc) +  2.3588(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3199/ 4138] Loss:  3.2775 =  0.9642(loc) +  2.3133(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3299/ 4138] Loss:  3.4015 =  0.9828(loc) +  2.4187(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3399/ 4138] Loss:  3.3001 =  0.9595(loc) +  2.3405(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3499/ 4138] Loss:  3.2621 =  0.9501(loc) +  2.3120(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3599/ 4138] Loss:  3.2412 =  0.9250(loc) +  2.3162(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3699/ 4138] Loss:  3.0456 =  0.8505(loc) +  2.1951(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3799/ 4138] Loss:  3.2780 =  0.9277(loc) +  2.3503(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3899/ 4138] Loss:  3.1892 =  0.8839(loc) +  2.3053(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  3999/ 4138] Loss:  3.2421 =  0.9219(loc) +  2.3202(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "[Epoch   1][iter  4099/ 4138] Loss:  3.3495 =  0.9517(loc) +  2.3978(cls)                   || forward 0.01s, backward 0.05s || lr: 0.000100\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    optim_scheduler.step()\n",
    "    train(epoch, net, dataloader, optim_scheduler.get_lr()[0])    \n",
    "    torch.save(net.state_dict(), 'weights/ssd300_epoch_{:03d}.pth'.format(epoch))\n",
    "    \n",
    "print('done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
